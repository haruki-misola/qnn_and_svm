{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea033943",
   "metadata": {},
   "source": [
    "Kaggle上下载得到creditcard数据集。查看数据集的shape以及label。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11a79cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv(r'creditcard.csv')\n",
    "print(data.values.shape)\n",
    "print(data['Class'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8a31538",
   "metadata": {},
   "source": [
    "(284807, 31)\n",
    "0    284315\n",
    "1       492\n",
    "Name: Class, dtype: int64\n",
    "\n",
    "数据有30个特征，“normal”类样本有28W个，而“fraud”类特征仅492个，数据分布极不平衡。对于此类数据集，常用的方法有上采样和下采样。由于对“fraud”类样本进行上采样会导致数据量急剧增加，从而给量子神经网络增添极大的负担。因此采用下采样方法。\n",
    "\n",
    "首先将数据的特征与标签分离，并去除掉无关列“Time”:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6869800a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"creditcard.csv\").drop('Time',axis=1).values[:,0:-1]\n",
    "label = pd.read_csv(\"creditcard.csv\").values[:,-1].reshape(-1,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeaa970c",
   "metadata": {},
   "source": [
    "数据中包括有30个特征，对于量子神经网络来说负担过大，因此使用PCA对数据进行降维。从sklearn中导入PCA："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1c04e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=16)\n",
    "down_data = pca.fit_transform(data)\n",
    "print(down_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37b3e8bb",
   "metadata": {},
   "source": [
    "(284807, 16)\n",
    "\n",
    "降维成16个特征。从sklearn导入划分训练集和测试集的包，并将训练集中的“normal”类样本与“fraud”类样本分别提取出来："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "855d175a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "new_data = np.hstack((down_data,label))\n",
    "train, test = train_test_split(new_data,random_state=2,train_size=0.7)\n",
    "\n",
    "test_data = test[:,0:-1]\n",
    "test_label = test[:,-1]\n",
    "\n",
    "train_normal_data = train[train[:,-1] == 0][:,0:-1]\n",
    "train_normal_label = train[train[:,-1] == 0][:,-1]\n",
    "\n",
    "fraud_data = train[train[:,-1] == 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b14a75da",
   "metadata": {},
   "source": [
    "对“normal”类样本进行下采样，在这里使用聚类的方法，从聚成的若干类中分别提取出若干个样本，再与“fraud”类组成新的训练集。聚类的个数使用轮廓系数法确定："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cf70c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "n_clusters_range = range(2, 11)\n",
    "silhouette_scores = []\n",
    "for n_clusters in n_clusters_range:\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=0)\n",
    "    cluster_labels = kmeans.fit_predict(train_normal_data)\n",
    "    silhouette_scores.append(silhouette_score(train_normal_data, cluster_labels))\n",
    "\n",
    "best_n_clusters = n_clusters_range[np.argmax(silhouette_scores)]\n",
    "print(f\"最佳聚类数: {best_n_clusters}\")\n",
    "print(f\"“1”类样本个数：\" {len(fraud_data)})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75ef0439",
   "metadata": {},
   "source": [
    "最佳聚类数：2\n",
    "“1”类样本个数: 358\n",
    "\n",
    "将“normal”类样本制作成和“fraud”类样本相同的个数，因此从聚成的两类中，分别选择离聚类中心最近的179个样本，组成new_normal_data："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "278d4c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=2, random_state=0).fit(train_normal_data)\n",
    "\n",
    "labels = kmeans.labels_\n",
    "centers = kmeans.cluster_centers_\n",
    "\n",
    "distances = kmeans.transform(train_normal_data)\n",
    "\n",
    "n = len(fraud_data) / best_n_clusters\n",
    "indices = []\n",
    "new_normal_data = []\n",
    "for i in range(kmeans.n_clusters):\n",
    "    indices_i = np.argsort(distances[:,i])[:n]\n",
    "    indices.append(indices_i)\n",
    "\n",
    "for i in range(kmeans.n_clusters):\n",
    "    for idx in indices[i]:\n",
    "        new_normal_data.append(train_normal_data[idx])\n",
    "new_normal_data = np.array(new_normal_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1ea4abc",
   "metadata": {},
   "source": [
    "将得到的新的“normal”类样本与“fraud”类组成train_data，并进行归一化："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64040e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from pandas import DataFrame\n",
    "\n",
    "zero = np.zeros(358).reshape(-1,1)\n",
    "new_normal = np.hstack((new_normal_data,zero))\n",
    "\n",
    "train_data = np.vstack((new_normal,fraud_data))[:,0:-1]\n",
    "train_label = np.vstack((new_normal,fraud_data))[:,-1]\n",
    "\n",
    "da = StandardScaler().fit(train_data)\n",
    "train_data = da.transform(train_data)\n",
    "minmax_scale = MinMaxScaler((-1, 1)).fit(train_data)\n",
    "train_data = minmax_scale.transform(train_data)\n",
    "\n",
    "\n",
    "test_data = da.transform(test_data)\n",
    "test_data = minmax_scale.transform(test_data)\n",
    "\n",
    "d1 = DataFrame(train_data)\n",
    "d1.to_csv(r'train_data.csv',header=None,index=False)\n",
    "d2 = DataFrame(train_label)\n",
    "d2.to_csv(r'train_label.csv',header=None,index=False)\n",
    "d3 = DataFrame(test_data)\n",
    "d3.to_csv(r'test_data.csv',header=None,index=False)\n",
    "d4 = DataFrame(test_label)\n",
    "d4.to_csv(r'test_label.csv',header=None,index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4cdd5f9",
   "metadata": {},
   "source": [
    "构建一个简单的SVM模型："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f23423d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "clf = svm.SVC(kernel = 'rbf')\n",
    "clf.fit(train_data,train_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c24c9dc4",
   "metadata": {},
   "source": [
    "由于数据不平衡，我们使用accuracy、recall和confusion_matrix进行评价："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7494fa09",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import recall_score, accuracy_score, confusion_matrix\n",
    "\n",
    "\n",
    "p = clf.predict(test_data)\n",
    "labels = [0, 1] \n",
    "cm = confusion_matrix(test_label, p, labels=labels)\n",
    "print(cm)\n",
    "print(\"accuracy:\",accuracy_score(test_label,p))\n",
    "print(\"recall:\",recall_score(test_label,p))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af448390",
   "metadata": {},
   "source": [
    "[[81864  3445]\n",
    " [   13   121]]\n",
    "accuracy: 0.9595285746052924\n",
    "recall: 0.9029850746268657\n",
    "\n",
    "由于在训练数据时将“normal”类样本进行了下采样，虽然减少了数据量，加快了模型的训练速度，但也导致在测试模型时有3445个“normal”类样本分类错误。下采样的效果劣于上采样，优于不处理，但此对比并不在题目范围内，因此未放在notebook上。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e9cc370",
   "metadata": {},
   "source": [
    "使用量子神经网络构建分类模型。使用tensorflow_quantum建立QNN。首先将经典数据导入电路中，这里借鉴文献[1]的方法，设置一个阈值，并将大于阈值的经典数字置为1，反之置为0。若第i个数字为1，则在第i个qubit上加入一个X门，数字为0不采取任何动作:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b061a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_quantum as tfq\n",
    "import cirq\n",
    "import sympy\n",
    "\n",
    "thres = 0  #由于对数据进行处理时缩放到了-1，1的范围内，所以以0为阈值\n",
    "\n",
    "train_data_binary = np.array(train_data > thres, dtype=np.float32)\n",
    "test_data_binary = np.array(test_data > thres, dtype=np.float32)\n",
    "\n",
    "def convert(data):\n",
    "    values = np.ndarray.flatten(data)\n",
    "    qubits = cirq.GridQubit.rect(4, 4)   #16个特征，定义16个qubit\n",
    "    circuit = cirq.Circuit()\n",
    "    for i, value in enumerate(values):   #同时循环样本的序列和数值\n",
    "        if value == 1:\n",
    "            circuit.append(cirq.X(qubits[i]))   #当数值为1时，记录其序列，并在该序列对应的qubit上作用一个X门\n",
    "    return circuit\n",
    "\n",
    "train_data_circ = [convert(i) for i in train_data_binary]\n",
    "test_data_circ = [convert_to_circuit(j) for j in test_data_binary]\n",
    "\n",
    "#转换为tensor\n",
    "train_data_tensor = tfq.convert_to_tensor(train_data_circ)\n",
    "test_data_tensor = tfq.convert_to_tensor(test_data_circ)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d17fa726",
   "metadata": {},
   "source": [
    "构建量子网络层："
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
