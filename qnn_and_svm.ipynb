{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea033943",
   "metadata": {},
   "source": [
    "Kaggle上下载得到creditcard数据集。查看数据集的shape以及label。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11a79cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv(r'creditcard.csv')\n",
    "print(data.values.shape)\n",
    "print(data['Class'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8a31538",
   "metadata": {},
   "source": [
    "(284807, 31)\n",
    "0    284315\n",
    "1       492\n",
    "Name: Class, dtype: int64\n",
    "\n",
    "数据有30个特征，“normal”类样本有28W个，而“fraud”类特征仅492个，数据分布极不平衡。对于此类数据集，常用的方法有上采样和下采样。由于对“fraud”类样本进行上采样会导致数据量急剧增加，从而给量子神经网络增添极大的负担。因此采用下采样方法。\n",
    "\n",
    "首先将数据的特征与标签分离，并去除掉无关列“Time”:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6869800a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"creditcard.csv\").drop('Time',axis=1).values[:,0:-1]\n",
    "label = pd.read_csv(\"creditcard.csv\").values[:,-1].reshape(-1,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeaa970c",
   "metadata": {},
   "source": [
    "数据中包括有30个特征，对于量子神经网络来说负担过大，因此使用PCA对数据进行降维。从sklearn中导入PCA："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1c04e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=16)\n",
    "down_data = pca.fit_transform(data)\n",
    "print(down_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37b3e8bb",
   "metadata": {},
   "source": [
    "(284807, 16)\n",
    "\n",
    "降维成16个特征。从sklearn导入划分训练集和测试集的包，并将训练集中的“normal”类样本与“fraud”类样本分别提取出来："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "855d175a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "new_data = np.hstack((down_data,label))\n",
    "train, test = train_test_split(new_data,random_state=2,train_size=0.7)\n",
    "\n",
    "test_data = test[:,0:-1]\n",
    "test_label = test[:,-1]\n",
    "\n",
    "train_normal_data = train[train[:,-1] == 0][:,0:-1]\n",
    "train_normal_label = train[train[:,-1] == 0][:,-1]\n",
    "\n",
    "fraud_data = train[train[:,-1] == 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b14a75da",
   "metadata": {},
   "source": [
    "对“normal”类样本进行下采样，在这里使用聚类的方法，从聚成的若干类中分别提取出若干个样本，再与“fraud”类组成新的训练集。聚类的个数使用轮廓系数法确定："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cf70c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "n_clusters_range = range(2, 11)\n",
    "silhouette_scores = []\n",
    "for n_clusters in n_clusters_range:\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=0)\n",
    "    cluster_labels = kmeans.fit_predict(train_normal_data)\n",
    "    silhouette_scores.append(silhouette_score(train_normal_data, cluster_labels))\n",
    "\n",
    "best_n_clusters = n_clusters_range[np.argmax(silhouette_scores)]\n",
    "print(f\"最佳聚类数: best_n_clusters\")\n",
    "print(f\"“1”类样本个数：len(fraud_data\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75ef0439",
   "metadata": {},
   "source": [
    "最佳聚类数：2\n",
    "“1”类样本个数: 358\n",
    "\n",
    "将“normal”类样本制作成和“fraud”类样本相同的个数，因此从聚成的两类中，分别选择离聚类中心最近的179个样本，组成new_normal_data："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "278d4c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=2, random_state=0).fit(train_normal_data)\n",
    "\n",
    "labels = kmeans.labels_\n",
    "centers = kmeans.cluster_centers_\n",
    "\n",
    "distances = kmeans.transform(train_normal_data)\n",
    "\n",
    "n = len(fraud_data) / best_n_clusters\n",
    "indices = []\n",
    "new_normal_data = []\n",
    "for i in range(kmeans.n_clusters):\n",
    "    indices_i = np.argsort(distances[:,i])[:n]\n",
    "    indices.append(indices_i)\n",
    "\n",
    "for i in range(kmeans.n_clusters):\n",
    "    for idx in indices[i]:\n",
    "        new_normal_data.append(train_normal_data[idx])\n",
    "new_normal_data = np.array(new_normal_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1ea4abc",
   "metadata": {},
   "source": [
    "将得到的新的“normal”类样本与“fraud”类组成train_data，并进行归一化："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64040e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from pandas import DataFrame\n",
    "\n",
    "zero = np.zeros(358).reshape(-1,1)\n",
    "new_normal = np.hstack((new_normal_data,zero))\n",
    "\n",
    "train_data = np.vstack((new_normal,fraud_data))[:,0:-1]\n",
    "train_label = np.vstack((new_normal,fraud_data))[:,-1]\n",
    "\n",
    "da = StandardScaler().fit(train_data)\n",
    "train_data = da.transform(train_data)\n",
    "minmax_scale = MinMaxScaler((-1, 1)).fit(train_data)\n",
    "train_data = minmax_scale.transform(train_data)\n",
    "\n",
    "\n",
    "test_data = da.transform(test_data)\n",
    "test_data = minmax_scale.transform(test_data)\n",
    "\n",
    "# d1 = DataFrame(train_data)\n",
    "# d1.to_csv(r'train_data.csv',header=None,index=False)\n",
    "# d2 = DataFrame(train_label)\n",
    "# d2.to_csv(r'train_label.csv',header=None,index=False)\n",
    "# d3 = DataFrame(test_data)\n",
    "# d3.to_csv(r'test_data.csv',header=None,index=False)\n",
    "# d4 = DataFrame(test_label)\n",
    "# d4.to_csv(r'test_label.csv',header=None,index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4cdd5f9",
   "metadata": {},
   "source": [
    "构建一个简单的SVM模型："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f23423d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "#################可在此处直接导入处理好的数据######################\n",
    "# train_data = np.genfromtxt(r'train_data.csv', delimiter=',')\n",
    "# test_data = np.genfromtxt(r'test_data.csv', delimiter=',')\n",
    "# train_label = np.genfromtxt(r'train_label.csv', delimiter=',')\n",
    "# test_label = np.genfromtxt(r'test_label.csv', delimiter=',')\n",
    "clf = svm.SVC(kernel = 'rbf')\n",
    "clf.fit(train_data,train_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c24c9dc4",
   "metadata": {},
   "source": [
    "由于数据不平衡，我们使用accuracy、recall和confusion_matrix进行评价："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7494fa09",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import recall_score, accuracy_score, confusion_matrix\n",
    "\n",
    "\n",
    "p = clf.predict(test_data)\n",
    "labels = [0, 1] \n",
    "cm = confusion_matrix(test_label, p, labels=labels)\n",
    "print(cm)\n",
    "print(\"accuracy_svm:\",accuracy_score(test_label,p))\n",
    "print(\"recall_svm:\",recall_score(test_label,p))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af448390",
   "metadata": {},
   "source": [
    "[[81864  3445]\n",
    " [   13   121]]\n",
    "accuracy_svm: 0.9595285746052924\n",
    "recall_svm: 0.9029850746268657\n",
    "\n",
    "由于在训练数据时将“normal”类样本进行了下采样，虽然减少了数据量，加快了模型的训练速度，但也导致在测试模型时有3445个“normal”类样本分类错误。下采样的效果劣于上采样，优于不处理，但此对比并不在题目范围内，因此未放在notebook上。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e9cc370",
   "metadata": {},
   "source": [
    "使用量子神经网络构建分类模型。使用tensorflow_quantum建立QNN。首先将经典数据导入电路中，这里借鉴文献[1]的方法，设置一个阈值，并将大于阈值的经典数字置为1，反之置为0。若第i个数字为1，则在第i个qubit上加入一个X门，数字为0不采取任何动作:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b061a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_quantum as tfq\n",
    "import cirq\n",
    "import sympy\n",
    "\n",
    "thres = 0  #由于对数据进行处理时缩放到了-1，1的范围内，所以以0为阈值\n",
    "\n",
    "train_data_binary = np.array(train_data > thres, dtype=np.float32)\n",
    "test_data_binary = np.array(test_data > thres, dtype=np.float32)\n",
    "\n",
    "def convert(data):\n",
    "    values = np.ndarray.flatten(data)\n",
    "    qubits = cirq.GridQubit.rect(4, 4)   #16个特征，定义16个qubit\n",
    "    circuit = cirq.Circuit()\n",
    "    for i, value in enumerate(values):   #同时循环样本的序列和数值\n",
    "        if value == 1:\n",
    "            circuit.append(cirq.X(qubits[i]))   #当数值为1时，记录其序列，并在该序列对应的qubit上作用一个X门\n",
    "    return circuit\n",
    "\n",
    "train_data_circ = [convert(i) for i in train_data_binary]\n",
    "test_data_circ = [convert(j) for j in test_data_binary]\n",
    "\n",
    "#转换为tensor\n",
    "train_data_tensor = tfq.convert_to_tensor(train_data_circ)\n",
    "test_data_tensor = tfq.convert_to_tensor(test_data_circ)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d17fa726",
   "metadata": {},
   "source": [
    "构建量子网络层："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e06b6f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_quantum_model():\n",
    "    data_qubits = cirq.GridQubit.rect(4, 4)  # 16个特征对应16个qubit\n",
    "    readout = cirq.GridQubit(-1, -1)         \n",
    "    circuit = cirq.Circuit()\n",
    "\n",
    "    # Prepare the readout qubit.\n",
    "    circuit.append(cirq.X(readout))\n",
    "    circuit.append(cirq.H(readout))\n",
    "\n",
    "    for i, qubit in enumerate(data_qubits):\n",
    "        symbol = sympy.Symbol('cx' + '-' + str(i))\n",
    "        circuit.append(cirq.CX(qubit, readout) ** symbol)\n",
    "        symbol = sympy.Symbol('cz' + '-' + str(i))\n",
    "        circuit.append(cirq.CZ(qubit, readout) ** symbol)  #构建参数电路，使用的量子门为cx和cz\n",
    "    circuit.append(cirq.H(readout))\n",
    "    return circuit, cirq.Z(readout)\n",
    "\n",
    "\n",
    "model_circuit, model_readout = create_quantum_model()\n",
    "model = tf.keras.Sequential([                              #构建tensorflow的sequential模型\n",
    "    tf.keras.layers.Input(shape=(), dtype=tf.string),\n",
    "    tfq.layers.PQC(model_circuit, model_readout),          #加入一个PQC量子层，量子电路为上述构建的model_circuit\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    loss=tf.keras.losses.BinaryCrossentropy(),             #二分类模型使用二次交叉熵\n",
    "    optimizer=tf.keras.optimizers.Adam(),\n",
    "    metrics=['accuracy'])                                  #训练模型的两类样本数量相等，因此使用accuracy\n",
    "\n",
    "model.fit(                                                 #fit模型\n",
    "      train_data_tensor, train_label,\n",
    "      batch_size=8,\n",
    "      epochs=10,                                           #云上jupyterhub内存有限，因此仅迭代10次\n",
    "      verbose=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31c02f33",
   "metadata": {},
   "source": [
    "训练模型的过程如下：\n",
    "\n",
    "Epoch 1/10\n",
    "90/90 [==============================] - 14s 157ms/step - loss: 0.6327 - accuracy: 0.6187\n",
    "Epoch 2/10\n",
    "90/90 [==============================] - 14s 157ms/step - loss: 0.6067 - accuracy: 0.6858\n",
    "Epoch 3/10\n",
    "90/90 [==============================] - 14s 158ms/step - loss: 0.5851 - accuracy: 0.7444\n",
    "Epoch 4/10\n",
    "90/90 [==============================] - 14s 153ms/step - loss: 0.5642 - accuracy: 0.7737\n",
    "Epoch 5/10\n",
    "90/90 [==============================] - 14s 157ms/step - loss: 0.5441 - accuracy: 0.7877\n",
    "Epoch 6/10\n",
    "90/90 [==============================] - 14s 157ms/step - loss: 0.5227 - accuracy: 0.7989\n",
    "Epoch 7/10\n",
    "90/90 [==============================] - 14s 154ms/step - loss: 0.5047 - accuracy: 0.8003\n",
    "Epoch 8/10\n",
    "90/90 [==============================] - 14s 158ms/step - loss: 0.4923 - accuracy: 0.8059\n",
    "Epoch 9/10\n",
    "90/90 [==============================] - 14s 155ms/step - loss: 0.4823 - accuracy: 0.8045\n",
    "Epoch 10/10\n",
    "90/90 [==============================] - 14s 158ms/step - loss: 0.4737 - accuracy: 0.8045\n",
    "\n",
    "10次的迭代精度基本也达到饱和。\n",
    "\n",
    "使用训练好的模型预测测试数据并计算效果："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d008863",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_qnn = model.predict(test_data_tensor)\n",
    "pre_qnn = (p > 0.5).astype(\"int32\")\n",
    "\n",
    "cm_qnn = confusion_matrix(test_label, p, labels=labels)\n",
    "print(cm_qnn)\n",
    "print(\"accuracy_qnn:\",accuracy_score(test_label,pre_qnn))\n",
    "print(\"recall_qnn:\",recall_score(test_label,pre_qnn))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7690bc95",
   "metadata": {},
   "source": [
    "[[83270  2039]\n",
    " [   43    91]]\n",
    "accuracy_qnn: 0.975632878059057\n",
    "recall_qnn: 0.6791044776119403\n",
    "\n",
    "与上述svm进行对比：\n",
    "\n",
    "[[81864  3445]\n",
    " [   13   121]]\n",
    "accuracy_svm: 0.9595285746052924\n",
    "recall_svm: 0.9029850746268657\n",
    "\n",
    "可以看到在accuracy上qnn略高于svm，但在对“欺诈”类样本的分类不如svm。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e103b046",
   "metadata": {},
   "source": [
    "参考文献：\n",
    "[1] https://arxiv.org/pdf/1802.06002.pdf"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}